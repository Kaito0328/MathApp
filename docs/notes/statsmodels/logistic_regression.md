# ロジスティック回帰

## 背景知識
二値分類で確率 P(y=1|x) = σ(β_0 + β^T x) を仮定する。対数尤度を最大化する。

## 入力例と出力例
- 入力: 特徴行列 X，ラベル y∈{0,1}，学習率 α，反復上限 max_iter
- 出力: 係数 β と予測確率・ラベル

## アルゴリズム
- 目的関数
	- 対数尤度 ℓ(β)=Σ [ y_i log σ(z_i) + (1−y_i) log(1−σ(z_i)) ]。正則化なら −λ||β||_1 または −(λ/2)||β||_2^2 を追加。

- 勾配法
	- 勾配 g=X^T (y−p)，学習率 α で β←β+α g。学習率は line search かスケジュールで減衰。

- ニュートン/IRLS（高速収束）
	1) W = diag(p_i(1−p_i))，z = Xβ + W^{−1}(y−p)。
	2) 重み付き最小二乗を解く: (X^T W X) Δ = X^T (y−p)。β←β+Δ。
	3) 数値は Cholesky/QR で解き，特異時はダンピングを入れる。

- 安定化
	- 予測 σ(z) は z をクリップ（|z| 上限）し，log(1+e^{−|z|}) 形式で安定評価。
	- 正則化 L2 は (X^T W X + λI) で自然に組み込める。L1 は座標降下や近接勾配で処理。

- 予測
	- predict_proba: p=σ(Xβ)。predict: しきい値で 0/1。

### 計算量と制約
- 勾配法: 1 反復 O(nnz(X))。IRLS: 1 反復で正規方程式解法 O(d^3)。
- 強い多重共線性は発散の原因。標準化・正則化・早期停止を用いる。
